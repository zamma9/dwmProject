{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-NN algorithm\n",
    "### project by Giulia Zammarchi for the Data and Web Mining 2023-24 course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** to demonstrate the theory clearly it will be used the same dataset used during lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-neighbors algorithm is a simple, instance-based learning (lazy learning) algorithm used to determine a class or to predict a numerical value.    \n",
    "It does not construct an internal model, but stores all the instances of the training data and the output is computed from a simple majority vote or an average value of the nearest neighbors points.   \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"intro.png\" alt=\"Descrizione dell'immagine\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can it be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN algorithm can be used for both classification and regression problems.   \n",
    "Where:\n",
    "- ***Classification*** problems are the ones in which the objective is to estimate the <ins>class</ins> of each input.    \n",
    "        - Ex. either an email is spam or not.\n",
    "- ***Regression*** problems are the ones in which the algorithm has to esimate a <ins>numerical value</ins> for each input.    \n",
    "        - Ex. the price of a house on the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has four steps, the first three are always the same\n",
    "1. ***Choose k value***\n",
    "2. ***Compute distance***\n",
    "3. ***Select Neighbors***  \n",
    "  \n",
    "And the fourth steps changes, we have  \n",
    " \n",
    "For <ins>classification</ins>:    \n",
    "\n",
    "4. a. ***Voting***  \n",
    "\n",
    "And for <ins>regression</ins>:  \n",
    "\n",
    "4. b. ***Mean computation***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose k value\n",
    "\n",
    "The value of $k$ determines the number of nearest neighbors to consider when making a prediction for a new instance.   \n",
    "The choice directly impacts the model's ability to generalize from the training data. A value that is too small or too large can lead to problems of underfitting or overfitting.  \n",
    "Let's see them closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small values of k\n",
    "\n",
    "<ins>PROS</ins>:  \n",
    "- Captures specific local trends,  \n",
    "- Quickly adapts to local changes,  \n",
    "- Handles intricate patterns well.  \n",
    "  \n",
    "<ins>CONS</ins>:  \n",
    "- Sensitive to noise and outliers, poor generalization, (overfitting)  \n",
    "- Influenced by specific training points,  \n",
    "- Outliers heavily impact predictions,  \n",
    "- Focuses on local patterns, missing broader trends.\n",
    "\n",
    "\n",
    "(CI STAREBBE BENE UNA SOLUZIONE O UNA RIGA DI SPIEGAZIONE ULTERIORE)  \n",
    "(ES CON CODICE: UN DATO FUORI POSTO CAMBIA LA PREDIZIONE DI UN DATO NELLA SUA AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large values of k\n",
    "<ins>PROS</ins>:  \n",
    "- Reduces outlier influence,  \n",
    "- Smoother, more robust decision boundaries,  \n",
    "- Often better on new data.  \n",
    "  \n",
    "<ins>CONS</ins>:  \n",
    "- Misses local patterns and variations,  \n",
    "- More resource-intensive for large datasets,  \n",
    "- Includes irrelevant data, reducing accuracy,  \n",
    "- Favors majority class. (in classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K value's choice\n",
    "\n",
    "1. Cross-Validation:  \n",
    "      - Use cross-validation to test various $k$ values, and choose the $k$ that provides the best    performance on validation data.\n",
    "\n",
    "2. Elbow Method:  \n",
    "      - Plot performance metrics against different $k$ values and look for an \"elbow\" where the performance improvement levels off.\n",
    "\n",
    "3. Balance Bias and Variance.\n",
    "\n",
    "4. Evaluate Computational Costs.\n",
    "      - Ensure the chosen $k$ is practical for your dataset size and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And many more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute distance  \n",
    "During this process the algorithm computes the distance between instances to identify the nearest neighbors.  \n",
    "Several distance metrics can be used, each with its own characteristics and suitability for different types of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "It calculates the straight line distance between two boints in euclidean space.  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \n",
    "$$\n",
    "It is suitable for continuous and numerical data where the scale of features is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance (L1 Distance)  \n",
    "It calculates the sum of absolute differences between coordinates of two points.  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\sum_{i=1}^n |x_i-y_i|\n",
    "$$\n",
    "\n",
    "It is useful for high-dimensional spaces and when the features represent different units or scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minokowski Distance  \n",
    "It's a generalization of the previous two methods. it's parameterized by $p$, and the $p$ determines the euclidean distance($p=2$) or the manhattan distance($p=1$).  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{\\frac{1}{p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev Distance  \n",
    "It measures the maximum absolute difference between the coordinates of two points.  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\text{max}_i \\  |x_i-y_i|\n",
    "$$\n",
    "It is suitable for problems where the maximum deviation in any one dimension is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Distance  \n",
    "It counts ne number of positions at which the corresponding elements are different.  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\sum_{i=1}^n \\Pi(x_i \\ne y_i)\n",
    "$$\n",
    "\n",
    "$\\Pi$ is the control value: it is 1 if $x_i\\ne y_i$ and 0 otherwise.  \n",
    "It is ideal for binary or categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance  \n",
    "It accounts for the correlations between variables and the scale of the data.  \n",
    "The formula:\n",
    "$$\n",
    "    d(x,y)=\\sqrt{(x-y)^T S^{-1}(x-y)}\n",
    "$$\n",
    "\n",
    "$S$ is the covariance matrix of the dataset.  \n",
    "It is useful for multivariate data where features are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Neighbors  \n",
    "This is the crucial step where the neighbors are selected. The accuracy and the effectiveness of the k-NN algorithm depend on identifying them.  \n",
    "Once the distances are computed, they get sorted and the smallest ones are selected. The nomer of instances depends obviously on the $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a. Voting\n",
    "Once the $k$ nearest neighbors have been selected, the next step in the algorithm is to make a prediction for the query instance. This is done through a process called \"voting\".  \n",
    "The voing can be weighted or by majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Voting\n",
    "The closer a neighbor is, the more weight is given to the distance. It is reversely proportional.  \n",
    "It leads to better performances but it is a bit more complex to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting\n",
    "Each of the neighbors gets an equal vote. The class with the most votes is assigned to the query instance.   \n",
    "It may not be the better option if some neighbors are closer then others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b. Mean Computation\n",
    "Where in classification the $k$ neighbors are selected and the final step is a choice, in regression the final step is a cumputation. The values are summed and divided by $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functioning and considerations are similar to those for the classification case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Considerations\n",
    "\n",
    "## What's good\n",
    "- Easy Implementation.\n",
    "\n",
    "- Dynamic Data Integration â†’ since the algorithm doesn't have a training phase, new data can be added at anytime without affecting the model.\n",
    "\n",
    "## What's not so good\n",
    "- Inefficient with larger datasets, it is computatinally expensive to calculate distances between a large number of instances.\n",
    "\n",
    "- All features must be properly scaled (normalized or standardized) to ensure fair distance measurements and effective model performance.\n",
    "\n",
    "- High-dimensional data complicates distance calculations, often leading to the \"curse of dimensionality\" where distances become less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
